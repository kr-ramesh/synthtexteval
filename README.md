## SynthEval: A Toolkit for Generating and Evaluating Synthetic Data Across Domains

<a href="https://colab.research.google.com/drive/1pM3Y0DGYAY2ocSismwOmshIJlWwC5WQW?usp=sharing" alt="Colab">
    <img src="https://colab.research.google.com/assets/colab-badge.svg" /></a>

## Contents
- [SynthEval: A Toolkit for Generating and Evaluating Synthetic Data Across Domains](#syntheval-a-toolkit-for-generating-and-evaluating-synthetic-data-across-domains)
    - [Contents](#contents)
    - [Introduction to SynthEval](#introduction-to-syntheval)
      - [Overview](#overview)
      - [Key Features of SynthEval](#key-features-of-syntheval)
    - [Repository Structure](#repository-structure)
    - [Installation Instructions](#installation-instructions)
      - [Setting up the environment](#setting-up-the-environment)
    - [Evaluation Pipeline](#evaluation-pipeline)
        - [Training the Model to Generate Synthetic Data](#training-the-model-to-generate-synthetic-data)
            - [Using Differential Privacy to Generate Synthetic Data](#using-differential-privacy-to-generate-synthetic-data) 
        - [Generating Synthetic Data](#generating-synthetic-data)
        - [Generating Descriptive Statistics](#generating-descriptive-statistics)
        - [Evaluating Downstream Utility](#evaluating-downstream-utility)
        - [Fairness Evaluation](#fairness-evaluation)
        - [Privacy Evaluation](#privacy-evaluation)
        - [Qualitative Evaluation](#qualitative-evaluation)
    - [Citations](#citations)


## Introduction to SynthEval

### Overview
SynthEval is an open-source library built to enable comprehensive evaluation of synthetic text generated by large language models (LLMs). With privacy concerns on the rise, especially in high-stakes domains, ensuring the utility, fairness, and privacy of systems trained on synthetic data is crucial. SynthEval is a unified framework designed for evaluating synthetic text across multiple dimensions, and integrates both the synthetic text generation and evaluation into a single pipeline. Additionally, it supports text generation using differentially private methods and enables qualitative assessments of this data, making it the first framework to offer a holistic approach to synthetic text evaluation.

### Key Features of SynthEval

- **Synthetic Text Generation** üìÑ: Enables controllable text generation using control codes to create targeted synthetic data based on specific training data attributes. It also supports differential privacy to ensure additional privacy protections when training text generators.
- **Downstream Utility Evaluation** üìå: Assesses the effectiveness of synthetic text for tasks like classification and coreference resolution, allowing for direct performance comparison between real and synthetic datasets.
- **Fairness Evaluation** ‚öñÔ∏è: Analyzes model performance across different subgroups to ensure fairness and detect any distributional biases in models trained on the generated text.
-  **Automated Open-Ended Text Evaluation** üéØ: Measures the quality of text using metrics such as Frechet Distance (FID), MAUVE Score, and perplexity to measure the quality and distributional differences between synthetic and real text corpora.
- **Visualization and Descriptive Text Analysis** üìä: Provides tools for visualizing and analyzing key text features like named entities, n-grams, TF-IDF, and topic modeling, providing deeper insights into text structure, diversity, and themes.
- **Privacy and Memorization** üîê: etects memorization in synthetic texts and language models through privacy evaluations, including canary attacks and entity-based metrics, to ensure sensitive information is not inadvertently reproduced and compliance with data regulations is followed.


## Repository Structure

The following is a condensed structure of our repository and the functionality it enables.

```
sdgeval
|____data # Dataset directory for data stored and used in the pipeline
|____descriptive
| |____descriptor.py # Module for descriptive analysis of text
|____downstream
| |____classify
| |____coref
|____fairness
|____generation
| |____controllable # Module for generating text with controllable generation
|____privacy
| |____canary
| |____privacy_metrics # Contains metrics for entity leakage and span memorization
|____qual
| |____metrics.py
| |____mauve_metric.py
| |____frechet.py
| |____perplexity.py
|____utils
| |____filtering.py # Filtering functionality provided in case the user wants to filter synthetic text
| |____utils.py

```

## Installation Instructions:

Clone the repository (the command below is for public repositories only)
```
git clone https://github.com/kr-ramesh/sdg-eval/
```

### Setting up the environment
Execute the following commands to install the dependencies for the environment to use the package and install it locally (Note: Need to publish this package to pip):
```
pip install -r requirements.txt
pip install -e .
```

3. Downloading data to test the toolkit (PhysioNet)

## Training the Model to Generate Synthetic Data

We provide functionality to train models to generate synthetic data using control codes, allowing for the creation of synthetic text with controllable attributes specified by the user. 

```
import sdgeval.generation.controllable.argument_utils as argument_utils
from sdgeval.generation.controllable.train_generator import train
from sdgeval.generation.controllable.testing_args import set_default_training_args, set_default_config_args

if __name__ == "__main__":
    train_args = set_default_training_args(dry_run=True)
    
    # Setting some default arguments for testing the training script
    model_args, data_args = set_default_config_args()
    privacy_args, lora_args= argument_utils.PrivacyArguments(), argument_utils.LoraArguments()
    # Disabling differentially private training 
    privacy_args.disable_dp = True
    data_args.dataset_name = # Set the name of the dataset here
    data_args.path_to_dataset = # Path to the CSV containing the dataset
    model_args.path_to_save_model = # Path to where the dataset and the model trained on this data will be saved
    data_args.control_field = # Name of the column in the dataframe corresponding to the control code
    data_args.text_field = # Name of the column in the dataframe corresponding to the text associated with the control code
    
    train(argument_utils.Arguments(train=train_args, privacy=privacy_args, model=model_args, data = data_args, lora=lora_args))
```

Alternatively, we provide a script to train the model in the ```generation/controllable``` subdirectory, which accepts the following arguments (some of which are optional) (further details regarding the arguments are specified in the ```generation/controllable``` subdirectory):

```
sh train.sh --model_name --path_to_save_the_model --disable_dp --epsilon_value --path_to_the_dataset --epochs --gradient_accumulation_steps --load_ckpt --path_to_load_model
```

### Using Differential Privacy to Generate Synthetic Data

Our training script provides the functionality to train models with differential privacy and LoRA, a parameter-efficient fine-tuning method. This can be toggled in the ```run-train.sh``` or directly in the ```train.sh``` script with the ```disable_dp``` and ```enable_lora``` arguments. The hyperparameters for the privacy budget and LoRA can also be specified in the ```train.sh``` script.

## Generating Synthetic Data

```
import sdgeval.generation.controllable.argument_utils as argument_utils
from sdgeval.generation.controllable.testing_args import set_default_training_args, set_default_config_args

if __name__ == "__main__":
    train_args = set_default_training_args(dry_run=False, dry_test_run = True)
    
    model_args, data_args = set_default_config_args()
    privacy_args, lora_args= argument_utils.PrivacyArguments(), argument_utils.LoraArguments()
    privacy_args.disable_dp = True
    data_args.dataset_name = # Name of the dataset
    data_args.path_to_test_dataset = # Path to the dataset/control codes that you want to use for generation
    model_args.path_to_load_model = # Path to the model to be loaded for inference
    data_args.control_field = # Name of the column in the dataframe corresponding to the control code
    data_args.text_field = # Name of the column in the dataframe corresponding to the text associated with the control code

    # Set to True when you want to perform inference
    model_args.inference = True
    model_args.num_return_seq = # Number of sequences to be generated for each control code

    
    inference(argument_utils.Arguments(train=train_args, privacy=privacy_args, model=model_args, data = data_args, lora=lora_args))
```

We also provide a script to run inference in the ```generation/controllable``` subdirectory. 

```
sh inf.sh --path_to_save_test_output --model_name --path_to_load_model --path_to_the_test_dataset --disable_dp
```

## Evaluation Pipeline:

Sample usage:


```
data = {
    'texts': [
        ...
    ]
}
df = pd.DataFrame(data)
```

## Generating Descriptive Statistics

The TextDescriptor class provides comprehensive text analysis capabilities, including named entity recognition, n-gram frequency analysis, TF-IDF computation, and topic modeling using LDA. Designed for evaluating synthetic and real text, it offers functionality to extract insights, visualize entity distributions, and save results for further analysis. Some of the functions can be called as follows:

```
from sdgeval.descriptive.descriptor import TextDescriptor
from sdgeval.descriptive.arguments import TextDescriptorArgs

desc_analyze = TextDescriptor(texts = data['texts'], args = TextDescriptorArgs(produce_plot=True))
desc_analyze.analyze_entities()
desc_analyze._topic_modeling()
desc_analyze._compute_tfidf()
desc_analyze._ngram_frequency()
```

## Evaluating Downstream Utility

### Classification

### Creating a Dataset

To prepare synthetic data for classification tasks, use create_classification_dataset to convert a CSV file into a structured dataset.

```
from sdgeval.utils.utils import create_classification_dataset

# Create a classification dataset
_, _, _ = create_classification_dataset(
    df, 
    label_column= # name of the label column, 
    output_json_path= # path to the output json file used for mapping the data to a numeric label,
    output_dir= # path to where the data splits will be saved, 
    multilabel=False, 
    train_ratio=0.7, test_ratio=0.15, val_ratio=0.15
)
```
### Training a Classifier

After creating the dataset, train a classifier using the sdgeval.downstream.classify module. We also provide a training script for this in the ```downstream/classify``` subdirectory.

```
from transformers import TrainingArguments as HfTrainingArguments
from sdgeval.downstream.classify.train_classifier import TrainingArguments, ModelArguments, Classifier, Arguments

if __name__ == "__main__":
    # Define training and model arguments
    train_args = TrainingArguments()
    model_args = ModelArguments()
    
    model_args.is_train = True  # Enable training
    model_args.text_field = # Name of the text column 
    model_args.label_field = # Name of the label column
    model_args.path_to_dataset = # Path to where the training set is
    model_args.n_labels = 5  # Number of labels for the classification task
    model_args.problem_type = 'single_label_classification'  # Set problem type

    args = Arguments(train=train_args, model=model_args)

    print("Initialization...")
    if args.model.is_train:
        print("Training:\n")
        obj = Classifier(args=args)
        obj.finetune_model()
```

### Testing the Classifier

After creating the dataset, we test our trained classifier using the sdgeval.downstream.classify module. We also provide a script for this in the ```downstream/classify``` subdirectory.

```
from transformers import TrainingArguments as HfTrainingArguments
from sdgeval.downstream.classify.train_classifier import TrainingArguments, ModelArguments, Classifier, Arguments

if __name__ == "__main__":
    # Define testing arguments
    train_args = TrainingArguments()
    model_args = ModelArguments()
    
    model_args.is_train = False  # Disable training
    model_args.is_test = True  # Enable testing
    model_args.path_to_model = # Path to trained model    
    model_args.text_field = # Name of the text column 
    model_args.label_field = # Name of the label column
    model_args.path_to_dataset = # Path to where the test set is
    model_args.n_labels = 5  # Number of labels for the classification task
    model_args.problem_type = 'single_label_classification'  # Set problem type
    model_args.retain_columns = ['Name', 'input_prompt']  # Keep specific columns

    args = Arguments(train=train_args, model=model_args)

    print("Initialization...")
    if args.model.is_test:
        print("Testing:\n")
        obj = Classifier(args=args)
        obj.test_model()
```

## Fairness Evaluation

Assessing fairness is crucial when evaluating synthetic text models. The analyze_group_fairness_performance() function helps analyze performance disparities across subgroups. The user can define any categorical column in their dataframe as the subgroup_type.


```
from sdgeval.fairness.metrics import analyze_group_fairness_performance

df = # Load dataframe containing model predictions from classification task

# Analyze group fairness
p_df, f_df = analyze_group_fairness_performance(
    df, 
    problem_type="single_label", # Set classification type
    num_classes= n_classes, # Number of classes
    subgroup_type= subgroup_type, # Attribute for subgroup analysis
)
```

- Performance analysis (p_df): Evaluates accuracy, precision, recall, and F1-score per subgroup.
- Fairness metrics (f_df): Measures disparities in model performance across different subgroups.

## Privacy Evaluation

Assuming the user already has access to a list of private entities from their original data, we can conduct a privacy evaluation as follows:

```
from sdgeval.privacy.privacy_metrics.metrics import entity_leakage

# Returns the overall percentage of leaked entities and a dictionary containing the entities leaked in each text
total_leakage, privacy_analysis = entity_leakage(paragraphs = # list of synthetic texts, entities = # list of private entities provided by the user, entity_leakage_result_path = # path to save the results)
```

The search_phrase() function helps identify occurrences of private entities in synthetically generated text and extracts surrounding context for analysis. It helps detect memorization in synthetically generated text by identifying instances where spans of text, including private entities, are regurgitated from the training data. By extracting these spans of text we can assess if private information is reproduced verbatim or subtly altered.

```
from sdgeval.privacy.privacy_metrics.metrics import search_phrase

fake_entities = # User-defined list of private entities

df = # Dataframe containing synthetically generated text
text_field = # Name of the column corresponding to the text field
max_window_len = # # Maximum number of words around each entity to extract its surrounding context
search_phrase(df = df, patterns = entity_list, max_window_len = max_window_len, text_field = text_field)
```

We also provide functionality to conduct canary-based evaluations for evaluating leakage in the generative model. Further details are provided in the ```privacy``` subdirectory.

## Qualitative Evaluation

We can evaluate the quality of synthetic text by comparing it against real-world samples using the Fr√©chet Inception Distance, MAUVE, and perplexity metrics.



```
from dataclasses import dataclass
from sdgeval.qual.metrics import QualEval
from sdgeval.qual.arguments import MauveArgs, LMArgs, FrechetArgs

# Define qualitative metric evaluation arguments
@dataclass
class Args:
    FrechetArgs: FrechetArgs
    MauveArgs: MauveArgs
    LMArgs: LMArgs

# Prepare evaluation dataframe containing synthetic and real samples
df = pd.DataFrame({
    'source':  # Synthetic text
    'reference': # Real text
})

args_qual = Args(FrechetArgs, MauveArgs, LMArgs)

qual_eval = QualEval(args_qual)

qual_eval.calculate_fid_score(df)     # Frechet Inception Distance (FID)
qual_eval.calculate_mauve_score(df)   # MAUVE Score for distribution similarity
qual_eval.calculate_perplexity(df)    # Perplexity score for fluency
```

