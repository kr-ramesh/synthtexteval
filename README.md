## SynthEval: A Toolkit for Generating and Evaluating Synthetic Data Across Domains

<a href="https://colab.research.google.com/drive/1pM3Y0DGYAY2ocSismwOmshIJlWwC5WQW?usp=sharing" alt="Colab">
    <img src="https://colab.research.google.com/assets/colab-badge.svg" /></a>

## Contents
- [SynthEval: A Toolkit for Generating and Evaluating Synthetic Data Across Domains](#syntheval-a-toolkit-for-generating-and-evaluating-synthetic-data-across-domains)
    - [Contents](#contents)
    - [Introduction to SynthEval](#introduction-to-syntheval)
      - [Overview](#overview)
      - [Key Features of SynthEval](#key-features-of-syntheval)
    - [Repository Structure](#repository-structure)
    - [Installation Instructions](#installation-instructions)
      - [Setting up the environment](#setting-up-the-environment)
    - [Evaluation Pipeline](#evaluation-pipeline)
        - [Generating Descriptive Statistics](#generating-descriptive-statistics)
        - [Evaluating Downstream Utility](#evaluating-downstream-utility)
            - [Classification](#classification)
            - [Coreference Resolution and Mention Annotation](#coreference-resolution-and-mention-annotation)
        - [Fairness Evaluation](#fairness-evaluation)
        - [Privacy Evaluation](#privacy-evaluation)
        - [Qualitative Evaluation](#qualitative-evaluation)
    - [Training the Model to Generate Synthetic Data](#training-the-model-to-generate-synthetic-data)
        - [Using Differential Privacy to Generate Synthetic Data](#using-differential-privacy-to-generate-synthetic-data) 
    - [Generating Synthetic Data](#generating-synthetic-data)
    - [Citations](#citations)


## Introduction to SynthEval

### Overview
SynthEval is an open-source library built to enable comprehensive evaluation of synthetic text generated by large language models (LLMs). With privacy concerns on the rise, especially in high-stakes domains, ensuring the utility, fairness, and privacy of systems trained on synthetic data is crucial. SynthEval is a unified framework designed for evaluating synthetic text across multiple dimensions, and integrates both the synthetic text generation and evaluation into a single pipeline. Additionally, it supports text generation using differentially private methods and enables qualitative assessments of this data, making it the first framework to offer a holistic approach to synthetic text evaluation.

### Key Features of SynthEval

- **Synthetic Text Generation** üìÑ: Enables controllable text generation using control codes to create targeted synthetic data based on specific training data attributes. It also supports differential privacy to ensure additional privacy protections when training text generators.
- **Downstream Utility Evaluation** üìå: Assesses the effectiveness of synthetic text for tasks like classification and coreference resolution, allowing for direct performance comparison between real and synthetic datasets.
- **Fairness Evaluation** ‚öñÔ∏è: Analyzes model performance across different subgroups to ensure fairness and detect any distributional biases in models trained on the generated text.
-  **Automated Open-Ended Text Evaluation** üéØ: Measures the quality of text using metrics such as Frechet Distance (FID), MAUVE Score, and perplexity to measure the quality and distributional differences between synthetic and real text corpora.
- **Visualization and Descriptive Text Analysis** üìä: Provides tools for visualizing and analyzing key text features like named entities, n-grams, TF-IDF, and topic modeling, providing deeper insights into text structure, diversity, and themes.
- **Privacy and Memorization** üîê: Detects memorization in synthetic outputs and language models through privacy evaluations that include canary attacks and entity-based metrics. This is to ensure sensitive information is not inadvertently reproduced and compliance with data regulations is followed.


## Repository Structure

The following is a condensed structure of our repository and the functionality it enables.

```
syntheval
|____data # Dataset directory for data stored and used in the pipeline
|____descriptive
| |____descriptor.py # Module for descriptive analysis of text
|____downstream
| |____classify
| |____coref
|____fairness
|____generation
| |____controllable # Module for generating text with controllable generation
|____privacy
| |____canary
| |____privacy_metrics # Contains metrics for entity leakage and span memorization
|____qual
| |____metrics.py
| |____mauve_metric.py
| |____frechet.py
| |____perplexity.py
|____utils
| |____filtering.py # Filtering functionality provided in case the user wants to filter synthetic text
| |____utils.py

```

## Installation Instructions:

Clone the repository (the command below is for public repositories only)
```
git clone https://github.com/kr-ramesh/syntheval/
```

### Setting up the environment
Execute the following commands to install the dependencies for the environment to use the package and install it locally (Note: Need to publish this package to pip):
```
pip install -r requirements.txt
pip install -e .
```

3. Downloading data to test the toolkit (PhysioNet)

## Evaluation Pipeline:

## Generating Descriptive Statistics

The TextDescriptor class provides comprehensive text analysis capabilities, including named entity recognition, n-gram frequency analysis, TF-IDF computation, and topic modeling using LDA. Designed for evaluating synthetic and real text, it offers functionality to extract insights, visualize entity distributions, and save results for further analysis. Some of the functions can be called as follows:

```python
from syntheval.descriptive.descriptor import TextDescriptor
from syntheval.descriptive.arguments import TextDescriptorArgs

desc_analyze = TextDescriptor(texts = texts, # A list of texts to analyze
                              args = TextDescriptorArgs(produce_plot=True), # Passes the arguments and the hyperparameters for the descriptor module
                              reference_texts = # (Optional) A list of reference texts, typically sourced from the real distribution.
                              )

# Example functionality:
desc_analyze.analyze_entities()
desc_analyze._topic_modeling()
desc_analyze._compute_tfidf()
desc_analyze._ngram_frequency()
```

## Evaluating Downstream Utility

### Classification

#### Generating silver annotations
If our synthetic data lacks label annotations, we can generate silver annotations for it using an existing pretrained model.

```python
from generate_silver_annotations import generate_silver_annotations

generate_silver_annotations(
    model_name = "bert-base-uncased",
    path_to_model = # path to model to use for silver annotations,
    n_labels = # number of labels,
    problem_type = # multiclass or multilabel classification,
    data_path = # path to data to annotate,
    text_column = # input text column,
    label_column = # label column,
    output_path = # path to the annotated CSV,
    ckpt_exists = True # set to True when loading from local files
)
```

#### Creating a dataset

To prepare synthetic data for classification tasks, use create_classification_dataset to convert a CSV file into a structured dataset.

```python
from syntheval.utils.utils import create_classification_dataset

# Create a classification dataset
_, _, _ = create_classification_dataset(
    df, 
    label_column= # name of the label column, 
    output_json_path= # path to the output json file used for mapping the data to a numeric label,
    output_dir= # path to where the data splits will be saved, 
    multilabel=False, 
    train_ratio=0.7, test_ratio=0.15, val_ratio=0.15
)
```
#### Training a classifier

After creating the dataset, train a classifier using the syntheval.downstream.classify module. We also provide a training script for this in the ```downstream/classify``` subdirectory. This script can also be used to test the classifier using the same module. Additional detailed documentation on the module, including its functionalities for testing with synthetic data and augmenting the training set with synthetic data, can be found in this subdirectory.

```python
# Command to execute the training script with the required parameters:
#
# Parameters:
#   --model_name: Name of the pre-trained model to use (e.g., bert-base-uncased)
#   --path_to_dataset: Path to the dataset directory (e.g., sst2)
#   --path_to_model: Directory where the trained model will be saved (e.g., models/bert-base-uncased-sst2)
#   --num_labels: The number of output labels for classification (e.g., 2 for binary classification)
#   --is_train: Whether to train the model (True or False)
#   --is_test: Whether to test the model (True or False)
#
# Usage:
#   sh train.sh <model_name> <path_to_dataset> <path_to_model> <num_labels> <is_train> <is_test>

# Example: 
sh train.sh bert-base-uncased "sst2" models/bert-base-uncased-sst2 2 True True
```

##

### Coreference Resolution and Mention Annotation

```python
temp_output_dir = './temp' # Define a temp output directory
model_dir = temp_output_dir + '/base_pretrained_model' # Path to where the base_pretrained_model is downloaded/saved
os.makedirs(temp_output_dir, exist_ok = True)
```

#### Download a pre-trained model (Instructions sourced from [here](https://github.com/yuvalkirstain/s2e-coref)):

```python
export MODEL_DIR=<model_dir>
curl -L https://www.dropbox.com/sh/7hpw662xylbmi5o/AAC3nfP4xdGAkf0UkFGzAbrja?dl=1 > temp_model.zip
unzip temp_model.zip -d $MODEL_DIR
rm -rf temp_model.zip
```

#### Generating silver annotations
In case our synthetic data does not have coreference annotations, we can generate silver annotations for this synthetic data using an existing pretrained model.

```python
from syntheval.downstream.coref.minimize_synth import minimize_file

synthetic_data_path = # Path to the synthetic data (a csv file)
output_path = "./temp" # Path to where outputs are saved
sample_size = 100 # Sampling a 100 samples from the synthetic data
minimize_file(synthetic_data_path, output_path, sample_size)
```

#### Fine-tuning and testing the model
Fine-tuning a model on these silver annotations and testing it on gold data (the paths can be specified in the arguments).
```python
from syntheval.downstream.coref.run_coref_comparison import coref_train
from syntheval.downstream.coref.arguments import set_default_coref_args

args = set_default_coref_args()
coref_train(args)
```

Alternative, we can run this with the following script provided in the ```downstream/coref``` subdirectory.

```python
python run_coref_comparison.py \
	--output_dir=$temp_output_dir \
        --model_type=longformer \
        --base_model_name_or_path=$model_dir \
        --tokenizer_name=allenai/longformer-large-4096 \
        --test_file=$test_file \
        --do_infer \
        --num_train_epochs=$num_train_epochs \
        --logging_steps=100 \
        --save_steps=1000 \
        --eval_steps=150 \
        --max_seq_length=4000 \
        --predict_file=$predict_file \
        --predict_file_write=$predict_file_write \
        --normalise_loss \
        --max_total_seq_len=4000 \
        --experiment_name=eval_model \
        --warmup_steps=5600 \
        --adam_epsilon=1e-6 \
        --head_learning_rate=3e-4 \
        --learning_rate=1e-5 \
        --adam_beta2=0.98 \
        --weight_decay=0.01 \
        --dropout_prob=0.3 \
        --save_if_best \
        --top_lambda=0.4  \
        --tensorboard_dir=$temp_output_dir/tb
```

## Fairness Evaluation

Assessing fairness is crucial when evaluating synthetic text models. The analyze_group_fairness_performance() function helps analyze performance disparities across subgroups. The user can define any categorical column in their dataframe as the subgroup_type.


```python
from syntheval.fairness.metrics import analyze_group_fairness_performance

df = # Load dataframe containing model predictions from classification task

# Analyze group fairness
p_df, f_df = analyze_group_fairness_performance(
    df, 
    problem_type="single_label", # Set classification type
    num_classes= n_classes, # Number of classes
    subgroup_type= subgroup_type, # Attribute for subgroup analysis
)
```

- Performance analysis (p_df): Evaluates accuracy, precision, recall, and F1-score per subgroup.
- Fairness metrics (f_df): Measures disparities in model performance across different subgroups.

## Privacy Evaluation

Assuming the user already has access to a list of private entities from their original data, we can conduct a privacy evaluation as follows:

```python
from syntheval.privacy.privacy_metrics.metrics import entity_leakage

# Returns the overall percentage of leaked entities and a dictionary containing the entities leaked in each text
total_leakage, privacy_analysis = entity_leakage(paragraphs = # list of synthetic texts, 
                                                entities = # list of private entities provided by the user, 
                                                entity_leakage_result_path = # path to save the results
                                                )
```

The search_phrase() function helps identify occurrences of private entities in synthetically generated text and extracts surrounding context for analysis. It helps detect memorization in synthetically generated text by identifying instances where spans of text, including private entities, are regurgitated from the training data. By extracting these spans of text we can assess if private information is reproduced verbatim or subtly altered.

```python
from syntheval.privacy.privacy_metrics.metrics import search_phrase

fake_entities = # User-defined list of private entities
df = # Dataframe containing synthetically generated text
text_field = # Name of the column corresponding to the text field
max_window_len = # # Maximum number of words around each entity to extract its surrounding context

search_phrase(df = df, patterns = entity_list, max_window_len = max_window_len, text_field = text_field)
```

We also provide functionality to conduct canary-based evaluations for evaluating leakage in the generative model. Further details are provided in the ```privacy``` subdirectory.

## Qualitative Evaluation

We can evaluate the quality of synthetic text by comparing it against real-world samples using the Fr√©chet Inception Distance, MAUVE, and perplexity metrics.



```python
from dataclasses import dataclass
from syntheval.qual.metrics import QualEval
from syntheval.qual.arguments import MauveArgs, LMArgs, FrechetArgs

# Define qualitative metric evaluation arguments
@dataclass
class Args:
    FrechetArgs: FrechetArgs
    MauveArgs: MauveArgs
    LMArgs: LMArgs

# Prepare evaluation dataframe containing synthetic and real samples
df = pd.DataFrame({
    'source':  # Synthetic text
    'reference': # Real text
})

args_qual = Args(FrechetArgs, MauveArgs, LMArgs)

qual_eval = QualEval(args_qual)

qual_eval.calculate_fid_score(df)     # Frechet Inception Distance (FID)
qual_eval.calculate_mauve_score(df)   # MAUVE Score for distribution similarity
qual_eval.calculate_perplexity(df)    # Perplexity score for fluency
```

## Training the Model to Generate Synthetic Data

We provide functionality to train models to generate synthetic data using control codes, allowing for the generation of synthetic text with controllable attributes specified by the user. We provide a script to train the model in the ```generation/controllable``` subdirectory, which accepts the following arguments (some of which are optional) (further details regarding the arguments are specified in the ```generation/controllable``` subdirectory):

```python
# Command to execute the training of the synthetic data generator.

#   Parameters:
#   --model_name           : The name of the pre-trained model (e.g., "princeton-nlp/Sheared-LLaMA-1.3B")
#   --path_to_save_model   : The directory to save the trained model (e.g., "/data/projects/syntheval/models/")
#   --disable_dp           : Disable Differential Privacy (true/false)
#   --epsilon_value        : The epsilon value for DP (used when DP is enabled)
#   --path_to_dataset      : Path to the dataset for training (e.g., "/data/datasets/wikipedia-biographies-v1-200.csv")
#   --epochs               : Number of epochs to train
#   --gradient_accumulation_steps : Number of gradient accumulation steps
#   --load_ckpt            : Whether to load a checkpoint for continuing training (true/false)
#   --path_to_load_model   : Path to the checkpoint model (if loading, else set to "")
#   --enable_lora          : Whether to enable LoRA (true/false)


# Usage:
#   sh train.sh <model_name> <path_to_save_the_model> <disable_dp> <epsilon_value> <path_to_the_dataset> <epochs> <gradient_accumulation_steps> <load_ckpt> <path_to_load_model> <enable_lora>

# Example:
sh train.sh "gpt2" "models/gpt2_DP_" true "inf" "dataset.csv" 5 1 false "" true
```


### Using Differential Privacy to Generate Synthetic Data

Our training script provides the functionality to train models with differential privacy and LoRA, a parameter-efficient fine-tuning method. This can be toggled in the ```run-train.sh``` or directly in the ```train.sh``` script with the ```disable_dp``` and ```enable_lora``` arguments. The hyperparameters for the privacy budget and LoRA can also be specified in the ```train.sh``` script.

```python
sh train.sh "gpt2" "models/gpt_DP_" false 8 "dataset.csv" 5 64 false "" true
```

## Generating Synthetic Data


We also provide a script to run inference in the ```generation/controllable``` subdirectory. 

```python
# Command to generate synthetic data from the trained generator model.

#   Parameters:
#   --path_to_save_test_output  : Directory to save the test output (e.g., "/data/projects/syntheval/test_outputs/")
#   --model_name                : The name of the pre-trained model (e.g., "princeton-nlp/Sheared-LLaMA-1.3B")
#   --path_to_load_model        : Path to the model checkpoint for loading (e.g., "/data/projects/syntheval/models/princeton_wiki_DP_8/")
#   --path_to_test_dataset      : Path to the dataset used for testing (e.g., "/data/datasets/wikipedia-biographies-v1-200.csv")
#   --disable_dp                : Whether to disable Differential Privacy (true/false)
#   --epsilon_value             : The epsilon value for DP (used when DP is enabled)
#   --enable_lora               : Whether to enable LoRA (true/false)

# Usage:
#   sh inf.sh <path_to_save_test_output> <model_name> <path_to_load_model> <path_to_test_dataset> <disable_dp> <epsilon_value> <enable_lora>

# Example:
sh inf.sh "inference.csv" "gpt2" "models/gpt2_DP_" "dataset/test.csv" true "inf" true
```

## Citations
