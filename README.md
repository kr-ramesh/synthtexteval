### SynthEval: A Toolkit for Generating and Evaluating Synthetic Data Across Domains

<a href="https://colab.research.google.com/drive/1pM3Y0DGYAY2ocSismwOmshIJlWwC5WQW?usp=sharing" alt="Colab">
    <img src="https://colab.research.google.com/assets/colab-badge.svg" /></a>

### Contents
- [SynthEval: A Toolkit for Generating and Evaluating Synthetic Data Across Domains](#syntheval-a-toolkit-for-generating-and-evaluating-synthetic-data-across-domains)
    - [Contents](#contents)
    - [Introduction to SynthEval](#introduction)
      - [Overview](#overview)
      - [Key Features of SynthEval](#key-features-of-syntheval)
      - [Jupyter Notebook](#jupyter-notebooks)
    - [Repository Structure](#repository-structure)
    - [Installation Instructions](#installation-instructions)
      - [Setting up the environment](#setting-up-the-environment)
    - [Evaluation Pipeline](#evaluation-pipeline)
        - [Training the Model to Generate Synthetic Data](#training-the-model-to-generate-synthetic-data)
            - [Using Differential Privacy to Generate Synthetic Data](#using-differential-privacy-to-generate-synthetic-data) 
        - [Generating Synthetic Data](#generating-synthetic-data)
        - [Generating Descriptive Statistics](#generating-descriptive-statistics)
        - [Evaluating Downstream Utility](#evaluating-downstream-utility)
        - [Fairness Evaluation](#fairness-evaluation)
        - [Privacy Evaluation](#privacy-evaluation)
        - [Qualitative Evaluation](#qualitative-evaluation)
    - [Citations](#citations)


### Introduction to SynthEval

SynthEval is an open-source library built to enable comprehensive evaluation of synthetic text generated by large language models (LLMs). With privacy concerns on the rise, especially in high-stakes domains, ensuring the utility, fairness, and privacy of systems trained on synthetic data is crucial. SynthEval is a unified framework designed for evaluating synthetic text across multiple dimensions, and integrates both the synthetic text generation and evaluation into a single pipeline. Additionally, it supports text generation using differentially private methods and enables qualitative assessments of this data, making it the first framework to offer a holistic approach to synthetic text evaluation.

### Installation Instructions:

1. Clone the repository (the command below is for public repositories only)
```
git clone https://github.com/kr-ramesh/sdg-eval/
```

2. Execute the following commands to install the dependencies for the environment to use the package and install it locally (Note: Need to publish this package to pip):
```
pip install -r requirements.txt
pip install -e .
```

3. Downloading data to test the toolkit (PhysioNet)

### Training the Model to Generate Synthetic Data

We provide functionality to train models to generate synthetic data using control codes, allowing for the creation of synthetic text with controllable attributes specified by the user. 

```
import sdgeval.generation.controllable.argument_utils as argument_utils
from sdgeval.generation.controllable.train_generator import train
from sdgeval.generation.controllable.testing_args import set_default_training_args, set_default_config_args

if __name__ == "__main__":
    train_args = set_default_training_args(dry_run=True)
    
    # Setting some default arguments for testing the training script
    model_args, data_args = set_default_config_args()
    privacy_args, lora_args= argument_utils.PrivacyArguments(), argument_utils.LoraArguments()
    # Disabling differentially private training 
    privacy_args.disable_dp = True
    data_args.dataset_name = # Set the name of the dataset here
    data_args.path_to_dataset = # Path to the CSV containing the dataset
    model_args.path_to_save_model = # Path to where the dataset and the model trained on this data will be saved
    data_args.control_field = # Name of the column in the dataframe corresponding to the control code
    data_args.text_field = # Name of the column in the dataframe corresponding to the text associated with the control code
    
    train(argument_utils.Arguments(train=train_args, privacy=privacy_args, model=model_args, data = data_args, lora=lora_args))
```

Alternatively, we provide a script to train the model in the ```generation/controllable``` directory, which accepts the following arguments (some of which are optional) (further details regarding the arguments are specified in the ```generation/controllable``` directory):

```
sh train.sh --model_name --path_to_save_the_model --disable_dp --epsilon_value --path_to_the_dataset --epochs --gradient_accumulation_steps --load_ckpt --path_to_load_model
```

#### Using Differential Privacy to Generate Synthetic Data

Our training script provides the functionality to train models with differential privacy and LoRA, a parameter-efficient fine-tuning method. This can be toggled in the ```run-train.sh``` or directly in the ```train.sh``` script with the ```disable_dp``` and ```enable_lora``` arguments. The hyperparameters for the privacy budget and LoRA can also be specified in the ```train.sh``` script.

### Generating Synthetic Data



### Evaluation Pipeline:

Sample usage:


```
data = {
    'texts': [
        ...
    ]
}
df = pd.DataFrame(data)
```

### Generating Descriptive Statistics

```
from sdgeval.descriptive.descriptor import Descriptor
from sdgeval.descriptive.arguments import DescriptorArgs
desc_analyze = Descriptor(data['texts'], DescriptorArgs(produce_plot=True))
```

### Evaluating Downstream Utility

### Fairness Evaluation

```
p_df, f_df = analyze_group_fairness_performance(df, problem_type = problem_type, num_classes = n)
```

### Privacy Evaluation

### Qualitative Evaluation

```
from sdgeval.qual.arguments import MauveArgs, LMArgs, FrechetArgs
from sdgeval.qual.mauve_metric import calculate_mauve_score
from sdgeval.qual.frechet import calculate_fid_score
from sdgeval.qual.perplexity import calculate_perplexity

result = calculate_mauve_score(df, MauveArgs)

result = calculate_fid_score(df, FrechetArgs)

result = calculate_perplexity(df, LMArgs)
```

